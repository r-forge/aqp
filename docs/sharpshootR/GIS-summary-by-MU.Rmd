---
output:
  html_document:
    mathjax: null
    jquery: null
    smart: no
---

```{r setup, echo=FALSE, results='hide', warning=FALSE}
# setup
library(knitr, quietly=TRUE)
library(printr, quietly=TRUE)
opts_chunk$set(message=FALSE, warning=FALSE, background='#F7F7F7', fig.align='center', fig.retina=2, dev='png', tidy=FALSE, verbose=FALSE)
options(width=100, stringsAsFactors=FALSE, cache=TRUE)
```

Computing GIS Summaries from Map Unit Polygons
==============================================
D.E. Beaudette
<br>
`r format(Sys.time(), "%Y-%m-%d")`


## Introduction
This tutorial decribes one approach to summarizing raster data (e.g. slope, aspect, PRISM data, etc.) according to delineations of a specific map unit. The examples outlined below are a good starting point for a more rigorous examination of map unit variability for the purposes of SDJR, quality control, or DSM project. You will need to adjust file paths to raster and vector data sources below, in order to follow along with your own data.

### Adapting Content to Your Data
Copy and paste blocks of code in this tutorial into a new R Studio script file (*ctrl + shift + n* makes a new file), edit, and then run. Running lines or blocks of code in an RStudio script file is as simple as moving the cursor to the line (or selecting a block) of code and press *ctrl + enter*.


## Setup R Environment
This step is only required the **first time** you open R. These packages will be available via `library()` in later sessions.

With a recent version of R (>= 2.15), it is possible to get all of the packages that this tutorial depends on via:
```{r install-deps, eval=FALSE}
# run these commands in the R console, only once
install.packages('plyr', dep=TRUE)
install.packages('reshape2', dep=TRUE)
install.packages('rgdal', dep=TRUE)
install.packages('raster', dep=TRUE)
install.packages('sharpshootR', dep=TRUE) # stable version from CRAN + dependencies
install.packages('sharpshootR', repos="http://R-Forge.R-project.org", type='source') # most recent copy from r-forge
```


## Map Unit Data
Map unit polyons can be loaded from shapefile, "file" geodatabase, or any other OGR-compatible data source. When working with large data sets it can be advantageous to export a subset of polygons using an external GIS application. In this example, the subsetting operations is performed in R. Be sure to adjust the file path below to point to a file containing map unit polygons.

Specifying paths in R must be done with either a single forward slash (/) or two backslashes (\\\\) between directories. Specifying the path to a shapefile or other data source also requires specialised syntax:

 * **file geodatabase** (E:/gis_data/FG_CA630.gdb)
    + dsn="path_to_file/file.gdb"
    + layer="feature_class_name"
    + example: `readOGR(dsn='E:/gis_data/FG_CA630.gdb', layer='ca630_a', encoding='OpenFileGDB', stringsAsFactors = FALSE)`
 * **shapefile** (E:/gis_data/ca630_a.shp)
    + dsn='path_to_parent_directory' (note trailing "/" is omitted)
    + layer='filename' (note trailing ".shp" is omitted)
    + example: `readOGR(dsn='E:/gis_data', layer='ca630_a', stringsAsFactors = FALSE)`

After loading and optionally filtering-out a single map unit, a unique ID is assigned to each polygon. This ID can later be used to reference polygons are associatiated with raster values outside of some ideal range.

```{r get-mu-data, fig.width=6, fig.height=7, results='hide'}
# load required packages
library(rgdal)
library(raster)
library(plyr)
library(reshape2)
library(sharpshootR)

# load map unit polygons
# Shapefile Example
# mu <-  readOGR(dsn='E:/gis_data/ca630', layer='ca630_a', stringsAsFactors = FALSE)

# File Geodatabase Example
mu <-  readOGR(dsn='E:/gis_data/ca630/FG_CA630_OFFICIAL.gdb', layer='ca630_a', encoding='OpenFileGDB', stringsAsFactors = FALSE)

# extract polygons for a single map unit ("MUSYM" attribute = "7089")
# note that column names in your data may be different
mu <- mu[which(mu$MUSYM == '7089'), ]

# add a unique polygon ID
mu$pID <- seq(from=1, to=length(mu))
```


## Raster Data
Raster data sources should be in a file format the can be read by the GDAL library; GeoTiFF, ArcGrid, or ERDAS formats are typically a safe option. Several raster data sources are used in this example:
 
 * "maat": 800 meter resolution, mean annual air temperature (1981--2010 PRISM) [UTM zone 10]
 * "ppt": 800 meter resolution, mean annual precipitation (1981--2010 PRISM) [UTM zone 10]
 * "wb": 800 meter resolution, mean of annual monthly differences between precipitation and potential ET (PRISM) [GCS NAD83]
 * "elev": 10 meter resolution, USGS DEM [UTM zone 10]
 * "slope": 10 meter resolution, 3x3 classic slope map derived from "elev" [UTM zone 10]
 * "aspect": 10 meter resolution, 3x3 classic aspect map derived from 10 meter "elev" [UTM zone 10]

When using your own data, adjust the names (e.g. "maat") and paths (e.g. "E:/gis_data/prism/tavg_1981_2010.tif") accordingly. The use of a single forward slash (/) to separate directories is suggested.
```{r load-raster-data-1}
raster.list <- list(maat=raster('E:/gis_data/prism/tavg_1981_2010.tif'), 
                    ppt=raster('E:/gis_data/prism/ppt_mm_1981_2010.tif'),
                    wb=raster('E:/gis_data/prism/annual_waterbudget800.tif'),
                    elev=raster('E:/gis_data/ca630/ca630_elev/hdr.adf'),
                    slope=raster('E:/gis_data/ca630/ca630_slope/hdr.adf'),
                    aspect=raster('E:/gis_data/ca630/aspect_ca630/hdr.adf')
)
```


Depending on the number of raster files, resolution, extent, and amount of memory on your workstation, it may be possible to load all of the rasters into RAM. It is worth trying because the performance gain can be significant. If you do not have enough memory, consider closing other applications and trying again.
```{r load-raster-data-2}
# estimate required RAM
est <- sum(sapply(raster.list, ncell) * 1.1) / 1000 / 1000
# load into RAM, if the estimated size of all rasters is smaller than available memory
if(est < memory.size())
  raster.list <- lapply(raster.list, readAll)
```


## Generate Sampling Points
As long as within-polygon variability (elevation, slope, aspect, etc.) is realitvely low, it is possible to generate reasonable summaries of these values using a subset of those pixels that fall within each polygon. Sampling each polygon at a constant density (e.g. 1 sample per ac.) ensures that summaries of raster data are not biased by polygon size. It is important to select a sampling density based extent and size of map unit delineations relative to raster data resolution. For example, a lower density (1 sample per 10 ac.) would be sufficient for summarizing broad-scale variation in climate, while a higher density (2 samples per ac.) would be required to capture finer-scale variation in slope.

Sampling of map unit polygons is performed with the `constantDensitySampling()` function from the sharpshootR package. Relevant arguments to this function include:

 * `n.pts.per.ac`: sampling density expressed as points / ac.
 * `min.samples`: minimum number of samples required per polygon
 * `polygon.id`: column containing a unique polygon ID
 

```{r generate-sampling-points, fig.width=6, fig.height=5.5}
# sample each polygon at a constant density, 1 point per acre
s <- constantDensitySampling(mu, n.pts.per.ac=1, min.samples=1, polygon.id='pID')

# extract a single polygon and associated samples
# in this case, the second polygon
p.example <- mu[which(mu$pID == 2), ]
s.example <- s[which(s$pID == 2), ]

# graphical check of a single polygon
par(mar=c(1,1,4,1))
# plotting the raster named "slope", you may need to adjust depending on your raster data
plot(raster.list[['slope']], ext=extent(p.example), axes=FALSE)
plot(p.example, add=TRUE)
points(s.example, cex=0.75, pch=3, col='black')
box()
title('Sampling Points, Polygon #2\nSlope Raster')
```


## Extract Raster Data at Sample Points
The following code iterates over each raster, extracts values at sample points, and combines into a single object, `d`. Sample points are automatically transformed to the spatial reference system of each raster by the `extract()` function. Note that samples from "continuous" data sources (elev, slope, etc.) are split from "circular" data sources (aspect).
```{r extract-at-points, fig.width=6, fig.height=7}
# init a list to store results
l <- list()

for(i in seq_along(raster.list)) {
  i.name <- names(raster.list)[i]
  l[[i.name]] <- data.frame(value=extract(raster.list[[i]], s), pID=s$pID)
  }

# convert to DF and fix default naming of raster column
d <- ldply(l)
names(d)[1] <- 'variable'

# split into ratio-scale and circular stats
d.circ <- subset(d, subset=variable == 'aspect')
d <- subset(d, subset=variable != 'aspect')
```


## Summarize Raster Stats Over All Delineations
[Quantiles](https://en.wikipedia.org/wiki/Quantile) are a convenient and (mostly) assumption-free method for describing distributions. In this tutorial the 5th, 50th, and 95th percentiles are used to represent "low", "RV", and "high" values. [Quartile coefficient of dispersion](https://en.wikipedia.org/wiki/Quartile_coefficient_of_dispersion) (QCD).
```{r summarize-samples, fig.width=6, fig.height=7}
stats <- ddply(d, 'variable', function(i) {
  # compute 5th, 50th, and 95th percentiles
  q <- quantile(i$value, probs=c(0.05, 0.5, 0.95), na.rm = TRUE)
  # round
  q <- round(q)
  q2 <- quantile(i$value, probs=c(0.25, 0.75), na.rm=TRUE)
  QCD <- (q2[2] - q2[1]) / (q2[2] + q2[1])
  res <- data.frame(t(q))
  if(nrow(res) > 0)
    res$QCD <- QCD
  # assign reasonable names (quantiles)
  names(res) <- c('q05', 'q50', 'q95', 'QCD')
  return(res)
  })

# print stats to screen
stats
```


## Summarize Raster Stats by Polygon
Sometimes it is useful to summarize raster data for *each* polygon. in this case the median value of each raster sampled is retained.
```{r summarize-by-poly-4}
# compute median values by polygon
poly.stats <- ddply(d, c('pID', 'variable'), function(i) {
  # q25, q50, q75
  q <- quantile(i$value, probs=c(0.05, 0.5, 0.95), na.rm = TRUE)
  QCD <- (q[3] - q[1]) / (q[3] + q[1])
  res <- data.frame(m=round(q[2]), QCD=round(QCD, 5))
  return(res)  
})

# convert to wide format, keeping median value
poly.stats.wide.1 <- dcast(poly.stats, pID ~ variable, value.var = 'm')
# convert to wide format, keeping QCD
poly.stats.wide.2 <- dcast(poly.stats, pID ~ variable, value.var = 'QCD')

# add a suffix to variable names so that we can combine
names(poly.stats.wide.1)[-1] <- paste0(names(poly.stats.wide.1)[-1], '_med')
names(poly.stats.wide.2)[-1] <- paste0(names(poly.stats.wide.2)[-1], '_QCD')

# join median + QCD stats for each polygon
poly.stats.wide <- join(poly.stats.wide.1, poly.stats.wide.2, by='pID')

# check first 10 rows
head(poly.stats.wide, 10)
```

Save to CSV file for later use.
```{r save-poly-stats, eval=FALSE}
write.csv(poly.stats.wide, file='poly-stats.csv', row.names=FALSE)
```

Join polygon ID and raster statistics with original map unit polygon layer and save to SHP file.
```{r save-poly-stats-shp, eval=FALSE}
# join stats to map unit polygon attribute table
mu@data <- join(mu@data, poly.stats.wide, by='pID', type='left')
# save to file
writeOGR(mu, dsn='.', layer='polygons-with-stats', driver='ESRI Shapefile', overwrite_layer=TRUE)
```


## Summarize Aspect Angle Over All Delineations
Aspect angle is a circular value and thus requires a specialized approach to computing quantiles. A graphical depiction helps with interpretation.
```{r summarize-aspect, fig.width=6, fig.height=6}
par(mar=c(0,0,0,0))
circ.stats <- aspect.plot(d.circ$value, q=c(0.05, 0.5, 0.95), plot.title='xxx', pch=NA, bg='RoyalBlue', col='black', arrow.col=c('grey', 'red', 'grey'), stack=FALSE, p.bw=90)

print(round(circ.stats))
```



## Advanced
The following code chunks highlight some additional ways in which the sampled raster values can be used to perform quality control or gain additional insight into map unit variability.


Locate polygons with >15% of samples outside of 5th-95th percentiles.
```{r summarize-by-poly-2, fig.width=7, fig.height=7}
polygons.to.check <- ddply(d, 'variable', function(i) {
  
  # convert to values -> quantiles
  e.i <- ecdf(i$value)
  q.i <- e.i(i$value)
  # locate those samples outside of our 5th-95th range
  out.idx <- which(q.i < 0.05 | q.i > 0.95)
  
  ## TODO: may need to protect against no matching rows?
  tab <- sort(prop.table(table(i$pID[out.idx])), decreasing = TRUE)
  df <- data.frame(pID=names(tab), prop.outside.range=round(unlist(tab), 2))
  # keep only those with > 15% of samples outside of range
  df <- df[which(df$prop > 0.15), ] 

  return(df)
})

# print details on flagged polygons
polygons.to.check

# make an index to those polygons that require further investigation
check.idx <- which(mu$pID %in% unique(polygons.to.check$pID))

# graphical check
par(mar=c(1,1,4,1))
plot(mu)
if(length(check.idx) > 0)
  plot(mu[check.idx, ], add=TRUE, border=NA, col='red')
box()
title('Polygons with > 15% of samples\noutside of 5th-95th percentile range')
```


Save flagged polygons to SHP file; be sure to adjust file paths. Note: `dsn` is the path to parent directory ("." means current working directory), and `layer` is the name of the shape file components--no need to specify ".shp".
```{r summarize-by-poly-3, eval=FALSE}
writeOGR(mu[check.idx, ], dsn='.', layer='flagged_polygons', driver='ESRI Shapefile', overwrite_layer=TRUE)
```


Sort polygons based on similarity of sampled raster values.
```{r summarize-by-poly-5, eval=FALSE}
# additional libraries
library(cluster)
library(MASS)

# eval numerical distance
p.dist <- daisy(poly.stats.wide[, -1], stand=TRUE)
# map to 2D space via Sammon Mapping
p.sammon <- sammon(p.dist)

# plot
par(mar=c(1,1,1,1))
plot(p.sammon$points, type='n', axes=FALSE, asp=1)
text(p.sammon$points, labels = poly.stats.wide$pID, cex=0.5)
box()
```


## Next Time...
1. scoring polygon bases on deviation from slope class limits
2. use gdalUtils to extract specific map unit polygons from large geodatabases (requires working GDAL installation)
3. generalized approach for comparing map units
4. use polygon data from SDA

----------------------------
This document is based on `sharpshootR` version `r utils::packageDescription("sharpshootR", field="Version")`.

