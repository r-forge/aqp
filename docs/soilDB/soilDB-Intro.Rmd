<title>Soil Database Interface: the soilDB package</title>
# Soil Database Interface: the <tt>soilDB</tt> package

```{r setup, echo=FALSE, results='hide'}
# setup
opts_chunk$set(message=FALSE, warning=FALSE, background='#F7F7F7', dpi=100, fig.align='center', dev='CairoPNG', tidy=FALSE, verbose=FALSE)
options(width=100, stringsAsFactors=FALSE)
```


## Introduction
This package provides methods for extracting soils information from local PedonPC and AK Site databases (MS Access format), local NASIS databases (MS SQL Server), and the SDA web-service. Currently USDA-NCSS data sources are supported, however, there are plans to develop interfaces to outside systems such as the Global Soil Mapping project.

### Installation
With a recent version of R, it should be possible to get all of the packages that `soilDB` depends on via:
```{r ssoap-deps, eval=FALSE}
install.packages('soilDB', dep=TRUE)
install.packages('RCurl', dep=TRUE)
install.packages('XML', dep=TRUE)
install.packages("SSOAP", repos = "http://www.omegahat.org/R", type="source")
install.packages("XMLSchema", repos = "http://www.omegahat.org/R", type="source")
```



### R Notes
A basic understanding of R syntax, object structure, package management system, and programming style will greatly help with integration of the `soilDB` and `aqp` packages into standard workflows. Here are some reminders. Recall that online help documents can be accessed via :

```{r manual-pages, eval=FALSE}
# get the manual page for a known function
help(mean)
# get the manual page for a package
help(soilDB)
help(aqp)
# package demo
demo(aqp)
# use fuzzy matching to search for a topic or keyword
help.search(boxplot)
```

**Mini R Tutorial**
```{r R-notes, eval=FALSE}
# 1. objects: creation, structure, etc.
x <- 1:10       # 'x' is now a vector of integers 1 to 10 
y <- rnorm(10, mean=0, sd=1)  # 'y' is now a vector of 10 random numbers {mean=0, sd=1}
d <- data.frame(x, y) # d is a rectangular table with columns 'x' and 'y'

str(d) # check structure
class(d) # inspect object class
head(d) # view first 6 rows
rm(d, x, y) # clean-up by deleting these objects

# 2. most function are vectorized: i.e. automatic iteration
x <- 1:10
x + 1 # the '+' function knows about vectors, and recycles scalars accordingly

# 3. missing data can cause problems unless accounted for
x <- c(x, NA) # 'c()' is the concatonate function
# print the value by typing it into the R console followed by enter
x

mean(x) # result is NA, as most functions return NA in the presence of missing data
mean(x, na.rm=TRUE) # mean computed after removal of missing data
```


## Design

The `soilDB` package provides two layers of functionality for extracting data from file-based databases (PedonPC), relational databases (local NASIS), and online data sources (Soil Data Access). High-level functions `fetchPedonPC()` and `fetchNASIS()` query, combine, verify horizon logic, and return `SoilProfileCollection` class objects containing the most commonly used site/pedon/horizon data. Low-level functions such as `get_site_data_from_pedon_db()` and `get_hz_data_from_pedon_db()` extract specific data from a designated source, and return the results as a `data.frame` class object.

### High-Level Functions

**Fetch Pedons or DMU data from Local NASIS**
The `fetchNASIS()` function will load all pedons from the local NASIS database. Pedon records are returned for *all* pedons/DMU in the local database, regardless of the selected set. An ODBC connection named 'nasis_local' must **first** be established (details pending).

```{r local-nasis-demo, eval=FALSE}
library(soilDB)
# pedons
f <- fetchNASIS()

# DMU data
fc <- fetchNASIS_component_data()
```

**Fetch Pedons from PedonPC 5.x Database**
The `fetchPedonPC(dsn)` function will load all pedon records into the current R session, based on the value of `dsn`-- a path to a PedonPC version 5.x database.

```{r pedonPC-demo, eval=FALSE}
library(soilDB)
# this will only work if you have a PedonPC Database and Windows :(
dsn <- "S:/Service_Center/NRCS/pedon/pedon.accdb"
f <- fetchPedonPC(dsn)
```


**Fetch data from Soil Data Access (SDA) Web-Service**
The `SDA_query()` function submits a query (ANSI SQL) to the SDA web-service using the `SSOAP` package, parses the result, and returns a `data.frame` (rectangular table) object.
```{r sda-example-1, eval=FALSE}
# setup a query
q <- "select cokey, compname, comppct_r 
from component 
where compname = 'yolo' and majcompflag = 'Yes' "

# run the query
res <- SDA_query(q)
```


**Get map unit keys for a geographic area**
The `MUKEYS_by_ll_bbox()` function accepts a bounding box defined in GCS (longitude, latitude) NAD83 coordinates, and returns a vector of map unit keys. This vector can be used to generate a filtering criteria for queries submitted via `SDA_query()`.
```{r sda-example-2, eval=FALSE}
# define bounding box
b <- c(-120.9, 37.7, -120.8, 37.8)

# get map unit keys for this bbox
m <- MUKEYS_by_ll_bbox(b)
```


**Get map unit geometry for a geographic area**
The `mapunit_geom_by_ll_bbox()` function accepts a bounding box defined in GCS (longitude, latitude) NAD83 coordinates, and returns a `SpatialPolygonsDataFrame` class object of map unit polygons. Note that map unit polygons that *overlap* with the bounding box are returned, rather than the *intersection* of bounding box and polygons.
```{r sda-example-3, eval=FALSE}
# define bounding box
b <- c(-120.54,38.61,-120.41,38.70)

# fetch the results, may take about 10 seconds
x <- mapunit_geom_by_ll_bbox(b)
```


### Low-Level Functions

**Fetch Data from a PedonPC 5.x Database**
These functions can be used to extract only subsets of the data available in a PedonPC database, but are generally only useful for advanced users.

```{r pedonPC-low-level-functions, eval=FALSE}
# fetch only site data
site_data <- get_site_data_from_pedon_db(dsn)
# fetch only horizon data
hz_data <- get_hz_data_from_pedon_db(dsn)
# fetch only color data
color_data <- get_colors_from_pedon_db(dsn)
# fetch extended data:
# diagnostic horizons, fragment summary, texture modifiers, geomorphology, taxonomic history
extended_data <- get_extended_data_from_pedon_db(dsn)
```

**Fetch Data from a Local NASIS Database**
These functions can be used to extract only subsets of the data available in the local NASIS database, but are generally only useful for advanced users.

```{r local-nasis-low-level-functions, eval=FALSE}
# fetch only site data
site_data <- get_site_data_from_NASIS_db()
# fetch only horizon data
hz_data <- get_hz_data_from_NASIS_db()
# fetch only color data
color_data <- get_colors_from_NASIS_db()
# fetch extended data:
# diagnostic horizons, fragment summary, texture modifiers, geomorphology, taxonomic history
extended_data <- get_extended_data_from_NASIS_db()
```


## Examples

### Getting SSURGO data from SDA
A quick example of how to use the USDA-NRCS [soil data access query facility](http://sdmdataaccess.nrcs.usda.gov/) (SDA). The following code describes how to get component-level soils data for Yolo County (survey area CA113) and compute representative sub-order level classification for each map unit. This example requires an understanding of [SQL](http://en.wikipedia.org/wiki/SQL), [US Soil Taxonomy](http://en.wikipedia.org/wiki/USDA_soil_taxonomy) and the [SSURGO](http://soils.usda.gov/survey/geography/ssurgo/) database. While not the most efficient approach to the task described below, the code does illustrate several strategies for working with SSURGO data in R.

First, we need to setup some functions that will help summarize component data, within each map unit. These functions will be applied *chunk-wise* to blocks of data, using the [split-apply-combine](http://www.jstatsoft.org/v40/i01/paper) strategy.
```{r sda-ssurgo-query-example-1}
# compute total component percentages for the current block of components
f.sum <- function(i) {
   n <- nrow(i) # number of rows
   s <- sum(i$comppct_r) # total component percent
   return(data.frame(pct=s, n=n)) # return the results
}

# pick the largest suborder from within each map unit
f.largest <- function(i) {
   i.sorted <- i[order(i$pct, decreasing=TRUE), ] # sort largest -> smallest
   top.suborder <- i.sorted$taxsuborder[1] # get the largest suborder
   top.suborder.pct <- i.sorted$pct[1] # get the the corresponding component percent
   return(data.frame(suborder=top.suborder, pct=top.suborder.pct)) # return results
}
```

Load required libraries, setup a query, submit to SDA, and process the results.
```{r sda-ssurgo-query-example-2, results='hide'}
# load libraries
library(soilDB)
library(plyr)

# get map unit-level data
q.1 <- "SELECT mukey, muacres
FROM legend
INNER JOIN mapunit ON mapunit.lkey = legend.lkey
WHERE legend.areasymbol = 'CA113'"

# get component-level data
q.2 <- "SELECT 
component.mukey, cokey, comppct_r, compname, taxclname, taxorder, taxsuborder, taxgrtgroup, taxsubgrp
FROM legend
INNER JOIN mapunit ON mapunit.lkey = legend.lkey
LEFT OUTER JOIN component ON component.mukey = mapunit.mukey
WHERE legend.areasymbol = 'CA113'"

# submit queries
mu <- SDA_query(q.1)
co <- SDA_query(q.2)

# tabulate percentage of suborder-level taxa within each map unit (mukey)
comp.suborder.sums <- ddply(co, .(mukey, taxsuborder), f.sum, .progress='text')

# keep the largest suborder, and its associated total percentage
comp.suborder <- ddply(comp.suborder.sums, .(mukey), f.largest, .progress='text')

# join the largest suborder (by map unit) to mu
x <- join(mu, comp.suborder, by='mukey')

# get an effective map unit area via: muacres * pct/100
x$effective.area <- with(x, muacres * pct / 100)
```

Compute the acreage of each suborder within Yolo county, round to the nearest integer, and sort. A spatially-explicit approach would be required for multi-survey area scenarios or an irregular chunk of a single survey.
```{r sda-ssurgo-query-example-3}
sort(round(tapply(x$effective.area, x$suborder, sum)))
```


### The `Gopheridge` Sample Dataset
The `gopheridge` sample dataset is very similar to the type of data returned from `fetchNASIS()` or `fetchPedonPC()`. The following demonstration is geared towards intermediate users of R and who are familiar with the classes and methods defined by the `aqp` package. Before proceeding it may be helpful to review the `aqp` manual pages: `library(aqp) ; help(aqp)`, or the [AQP kick-start](http://casoilresource.lawr.ucdavis.edu/drupal/node/1028).

Open R, and setup the environment by loading packages and the sample dataset.
```{r gopheridge-demo-1, eval=TRUE}
library(soilDB)
library(Hmisc) # lots of helper functions in here

# load example dataset
data(gopheridge)

# what kind of object is this?
class(gopheridge)

# print method of SoilProfileCollection objects
# lots in there!
gopheridge
```


Generate some aggregates of horizon data, and save as site-level data.
```{r gopheridge-demo-2, eval=TRUE}
# compute soil depth using an in-line function, note that max(SPC) returns the bottom-most depth
# this may not be the same as 'soil depth' when Cr or R horizons are present
gopheridge$soil.depth <- profileApply(gopheridge, function(x) max(x))

# horizon-thickness weighted mean (beware of missing data!)
f.wt.prop <- function(x, prop) {h <- horizons(x); w <- with(h, hzdepb-hzdept); wtd.mean(h[[prop]], weights=w)}

# get hz mid-point of hz with max clay content
f.max.clay.depth <- function(x) {
	h <- horizons(x) 
	max.clay <- which.max(h$clay) 
	with(h[max.clay, ], (hzdept+hzdepb) / 2)
	}

# apply by-profile, returning a vector, assigned to new column in @site
gopheridge$wt.clay <- profileApply(gopheridge, f.wt.prop, prop='clay')
gopheridge$wt.rf <- profileApply(gopheridge, f.wt.prop, prop='total_frags_pct')
gopheridge$max.clay.depth <- profileApply(gopheridge, f.max.clay.depth)
```

Define functions for weighted-mean properties within the PCS or diagnostic horizon, then apply to each profile. Also extract diagnostic horizon thickness and top depths, then save as site-level data.
```{r gopheridge-demo-3, eval=TRUE}
# compute aggregate (wt.mean) clay within particle-size control section
# note that this requires conditional eval of data that may contain NA
# see ?slab and ?soil.slot for details on the syntax
f.pcs.prop <- function(x, prop) {
  # these are accessed from @site
  sv <- c(x$psctopdepth, x$pscbotdepth)
  # test for missing PCS data
  if(any(is.na(sv)))
    return(NA)

  # create formula from named property
  fm <- as.formula(paste('~', prop))
  # return just the (weighted) mean, accessed from @horizons
  s <- slab(x, fm, seg_vect=sv)$p.mean
  return(s)
}

# compute the weighted-mean of some property within a given diagnostic horizon
# note that this requires conditional eval of data that may contain NA
# see ?slab and ?soil.slot for details on the syntax
# note that function expects certain columns within 'x'
f.diag.wt.prop <- function(x, d.hz, prop) {
	# extract diagnostic horizon data
	d <- diagnostic_hz(x)
	# subset to the requested diagnostic hz
	d <- d[d$diag_kind == d.hz, ]
	# if missing return NA
	if(nrow(d) == 0)
		return(NA)
	
	# extract depths and check for missing
	sv <- c(d$featdept, d$featdepb)
	if(any(is.na(sv)))
		return(NA)
	
	# create formula from named property
	fm <- as.formula(paste('~', prop))
	# return just the (weighted) mean, accessed from @horizons
	s <- slab(x, fm, seg_vect=sv)$p.mean
	return(s)
}

# conditional eval of thickness of some diagnostic feature or horizon
# will return a vector of length(x), you can save to @site
f.diag.thickness <- function(x, d.hz) {
	# extract diagnostic horizon data
	d <- diagnostic_hz(x)
	# subset to the requested diagnostic hz
	d <- d[d$diag_kind == d.hz, ]
	# if missing return NA
	if(nrow(d) == 0)
		return(NA)
	
	# compute thickness
	thick <- d$featdepb - d$featdept
	return(thick)
}

# conditional eval of top hz depth of diagnostic feature or horizon
# will return a vector of length(x), you can save to @site
f.diag.top <- function(x, d.hz) {
	# extract diagnostic horizon data
	d <- diagnostic_hz(x)
	# subset to the requested diagnostic hz
	d <- d[d$diag_kind == d.hz, ]
	# if missing return NA
	if(nrow(d) == 0)
		return(NA)
	
	# return top depth
	return(d$featdept)
}


# compute wt.mean clay within PCS, save to @site
gopheridge$pcs.clay <- profileApply(gopheridge, f.pcs.prop, prop='clay')

# compute wt.mean clay within argillic horizon, save to @site
gopheridge$argillic.clay <- profileApply(gopheridge, f.diag.wt.prop, d.hz='argillic horizon', prop='clay')

# compute argillic hz thickness, save to @site
gopheridge$argillic.thick <- profileApply(gopheridge, f.diag.thickness, d.hz='argillic horizon')

# compute depth to paralithic contact, if present, save to @site
gopheridge$paralithic.contact.depth <- profileApply(gopheridge, f.diag.top, d.hz='paralithic contact')
```



Re-order the `gopheridge` profiles by presence/absence of paralithic contact, plot, and then annotate depth of Cr contact.
```{r basic_plotting, fig.width=10, fig.height=5, out.width='100%'}
# order by presence of paralithic contact and its depth
new.order <- order(gopheridge$paralithic.contact, gopheridge$paralithic.contact.depth)
# setup margins:
par(mar=c(0,0,0,0))
plot(gopheridge, name='hzname', plot.order=new.order)
# annotate paralithic contact with lines
x.pos <- 1:length(gopheridge)
lines(x.pos, gopheridge$paralithic.contact.depth[new.order], lty=2, lwd=2, col='black')
```

----------------------------
This document is based on `aqp` version `r utils::packageDescription("aqp", field="Version")` and `soilDB` version `r utils::packageDescription("soilDB", field="Version")`.
